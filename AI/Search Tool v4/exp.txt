#
# Select the problem type:
#    1. Numerical Optimization
#    2. TSP
   Enter the number (pType) : 1
#  여기 있는걸 건드리면 된다~ 1번 풀고프면 1로 바꾸고 저쟝~
#  하고싶은거 주석풀고~
#
#   Enter the name of the file : problem/Convex.txt
#   Enter the name of the file : problem/Griewank.txt
   Enter the name of the file : problem/Ackley.txt
#   Enter the name of the file : problem/tsp30.txt
#   Enter the name of the file : problem/tsp50.txt
#   Enter the name of the file : problem/tsp100.txt
#
# Select the search algorithm:
#  Hill Climbing algorithms:
#    1. Steepest-Ascent
#    2. First-Choice
#    3. Stochastic
#    4. Gradient Descent
#  Metaheuristic algorithms:
#    5. Simulated Annealing
   Enter the number (aType ) : 4
#
# If you are solving a function optimization problem,
#   what should be the step size for axis-parallel mutation?
   Mutation step size (delta ) : 0.01
#
# If your algorithm choice is 2 or 3,
#   what should be the number of consecutive iterations without improvement?
   Give the number of iterations (limitStuck) : 100
#
# If your algorithm choice is 4 (gradient descent),
# what should be the update step size and increment for derivative?
   Update rate for gradient descent (alpha) : 0.01
   Increment for calculating derivative (dx) : 10 ** (-4)
#
# If you want a random-restart hill climbing,
# enter the number of restart. 
# Enter 1 if you do not want a random-restart.
   Number of restarts (numRestart) : 10
#  몇번 새롭게 다시 할건지
#
# If you are running a metaheuristic algorithm,
#   what should be the total number of evaluations until temination?
  Enter the number (limitEval) : 50000
#
# Enter the total number of experiments
   Enter the number (numExp) : 5
# 실험을 반복하는 넘버
# 똑같은 실험을 반복
# 랜덤 리스타트는 알고리즘내에 들어가서 하는거.
# 더 큰틀 안에서 실험을 또 하는거.